<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Random Sample Le v.a. $X_1,&mldr;,X_n$ sono dette un campionamento (o random sample) di dimensione $n$ da una popolazione $f(x)$ se $X_1,&mldr;,X_n$ sono mutualmente indipendenti e se la densit√† marginale √® la stessa $f(x)$."><title>ü™¥ Alessandro Straziota's Notes</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://Alessandrostr95.github.io//icon.png><link href=https://Alessandrostr95.github.io/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://Alessandrostr95.github.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://Alessandrostr95.github.io/js/darkmode.ff3928c070aaa41dd4e6a0d10ab7e115.min.js></script>
<script src=https://Alessandrostr95.github.io/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://Alessandrostr95.github.io/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://Alessandrostr95.github.io/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://Alessandrostr95.github.io/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://Alessandrostr95.github.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://Alessandrostr95.github.io/",fetchData=Promise.all([fetch("https://Alessandrostr95.github.io/indices/linkIndex.12ce054d07db7354a1a118a75dec1a5f.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://Alessandrostr95.github.io/indices/contentIndex.068b8904bb64a00c9464ed48895ebd2f.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://Alessandrostr95.github.io",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://Alessandrostr95.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/Alessandrostr95.github.io\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://Alessandrostr95.github.io/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://Alessandrostr95.github.io/>ü™¥ Alessandro Straziota's Notes</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Unknown
<a href=https://github.com/Alessandrostr95/quartz/tree/hugo/content/ISTI/Random%20Sample.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#random-sample>Random Sample</a><ol><li><a href=#esempio---campionamento-di-esponenziali>Esempio - campionamento di esponenziali</a></li></ol></li><li><a href=#somme-di-variabili-aleatorie-di-un-campionamento>Somme di variabili aleatorie di un campionamento</a><ol><li><a href=#media-campionaria>Media campionaria</a></li><li><a href=#varianza-campionaria>Varianza campionaria</a></li><li><a href=#teorema-524>Teorema 5.2.4</a></li><li><a href=#lemma-525---media-e-lemma>Lemma 5.2.5 - media e lemma</a></li><li><a href=#teorema-526---propriet√†-importanti>Teorema 5.2.6 - Propriet√† importanti</a></li></ol></li><li><a href=#teorema-527---fgm-della-media-campionaria>Teorema 5.2.7 - FGM della media campionaria</a><ol><li><a href=#proof-3>Proof</a></li><li><a href=#esempio---normale>Esempio - Normale</a></li></ol></li><li><a href=#teorema-529---convolution-formula>Teorema 5.2.9. - Convolution formula</a><ol><li></li><li><a href=#esempio---somma-di-va-di-cauchy>Esempio - somma di v.a. di Cauchy</a></li></ol></li></ol></nav></details></aside><a href=#random-sample><h2 id=random-sample><span class=hanchor arialabel=Anchor># </span>Random Sample</h2></a><p>Le v.a. $X_1,&mldr;,X_n$ sono dette un <strong>campionamento</strong> (o <strong>random sample</strong>) di <strong>dimensione</strong> $n$ da una <strong>popolazione</strong> $f(x)$ se $X_1,&mldr;,X_n$ sono <strong>mutualmente indipendenti</strong> e se la
<a href=/ISTI/Distribuzioni-Multivariate/ rel=noopener class=internal-link data-src=/ISTI/Distribuzioni-Multivariate/>densit√† marginale</a> √® la stessa $f(x)$.</p><p>Alternativamente $X_1, &mldr;, X_n$ sono anche dette <strong>i.i.d.</strong> con <em>pdf</em> o <em>pmf</em> $f(x)$.</p><p>Osserviamo che se il campionamento $(X_1, &mldr;, X_n)$ ha
<a href=/ISTI/Distribuzioni-Multivariate/ rel=noopener class=internal-link data-src=/ISTI/Distribuzioni-Multivariate/>distribuzione congiunta</a> $f(x_1, &mldr;, x_n)$, per indipendenza avremo la seguente uguaglianza</p><p>$$f(x_1, &mldr;, x_n) = f(x_1) \cdot &mldr; \cdot f(x_n) = \prod_{i = 1}^{n} f(x_i)$$
Analogamente quando il campionamento dipende da un singolo <strong>parametro condiviso</strong> $\theta$.
$$f(x_1, &mldr;, x_n \vert \theta) = \prod_{i=1}^{n} f(x_i \vert \theta)$$</p><a href=#esempio---campionamento-di-esponenziali><h3 id=esempio---campionamento-di-esponenziali><span class=hanchor arialabel=Anchor># </span>Esempio - campionamento di esponenziali</h3></a><p>Sia $X_1,&mldr;,X_n$ un campionamento da una popolazione esponenziale $\text{Exp}(\beta)$.
Ovvero
$$X_i = e^{-x_i/\beta}$$
Supponendo quindi che $\beta$ √® il <strong>parametro</strong> sul quale il campionamento dipende, avremo quindi una
<a href=/ISTI/Distribuzioni-Multivariate/ rel=noopener class=internal-link data-src=/ISTI/Distribuzioni-Multivariate/>joint pdf</a> del tipo
$$f(x_1,&mldr;,x_n \vert \beta) = \prod_{i=1}^{n}f(x_i \vert \beta) = \prod_{i=1}^{n}\frac{1}{\beta}e^{-x_i/\beta} = \frac{1}{\beta^n}e^{-(x_1+&mldr;+x_n)/\beta}$$
Questa <em>pdf</em> pu√≤ essere molto utile per rispondere ad alcune domande.
Per esempio supponiamo che $X_i$ indichi il tempo (espresso in anni) entro il quale un circuito si rompe.
Perci√≤, qual √® la probabilit√† che tutti i circuiti sopravvivano pi√π di 2 anni?
$$\begin{align*}
P(X_1 > 2, &mldr;, X_n > 2)
&= \int_{2}^{\infty} \cdots \int_{2}^{\infty} f(x_1, &mldr;, x_n \vert \theta) , dx_1 \cdots dx_n\\&= \int_{2}^{\infty} \cdots \int_{2}^{\infty} \prod_{i=1}^{n}\frac{1}{\beta}e^{-x_i/\beta} , dx_1 \cdots dx_n\\&= \int_{2}^{\infty} \cdots \int_{2}^{\infty} \left[-e^{-x/\beta} \right]<em>{2}^{\infty} \prod</em>{i=2}^{n}\frac{1}{\beta}e^{-x_i/\beta} , dx_2 \cdots dx_n\\&= \int_{2}^{\infty} \cdots \int_{2}^{\infty} e^{-2/\beta} \prod_{i=2}^{n}\frac{1}{\beta}e^{-x_i/\beta} , dx_2 \cdots dx_n\\&amp;;;\vdots\\&= \left( e^{-2/\beta}\right)^n\\&= e^{-2n/\beta}
\end{align*}$$</p><p>Ricordando che $\beta$ √® il <strong>valore atteso</strong> di $\text{Exp}(\beta)$ (in questo caso in quanto tempo in media si rompe un circuito), avremo che se $\beta \approx n$ allora tale probabilit√† sar√† prossima ad 1.</p><hr><a href=#somme-di-variabili-aleatorie-di-un-campionamento><h2 id=somme-di-variabili-aleatorie-di-un-campionamento><span class=hanchor arialabel=Anchor># </span>Somme di variabili aleatorie di un campionamento</h2></a><p>Sia $X_1, &mldr;, X_n$ un
<a href=/ISTI/Random-Sample/ rel=noopener class=internal-link data-src=/ISTI/Random-Sample/>random sample</a> di dimensione $n$ di una generica popolazione, e sia $T(x_1, &mldr;, x_n)$ una funzione il cui dominio √® lo spazio di campionamento di $(X_1, &mldr;, X_n)$.</p><p>La v.a. (o vettore) $Y \sim T(X_1, &mldr;, X_n)$ √® chiamata <strong>statistica</strong>. ^46a026</p><p>La distribuzione di probabilit√† di una statistica $Y$ √® anche chiamata <strong>distribuzione campionaria</strong> (o <strong>sampling distribution</strong>).</p><a href=#media-campionaria><h3 id=media-campionaria><span class=hanchor arialabel=Anchor># </span>Media campionaria</h3></a><p>La <strong>media campionaria</strong> di un sampling $X_1, &mldr;, X_n$ √® semplicemente la <strong>media aritmetica</strong>
$$\overline{X} = \frac{X_1 + &mldr; + X_n}{n}$$</p><a href=#varianza-campionaria><h3 id=varianza-campionaria><span class=hanchor arialabel=Anchor># </span>Varianza campionaria</h3></a><p>La <strong>varianza campionaria</strong> di un sampling $X_1, &mldr;, X_n$ √® definita come
$$S^2 = \frac{\sum_{i=1}^{n}(X_i - \overline{X})^2}{n-1}$$</p><p>Analogamente la <strong>deviazione standard campionaria</strong> √® definita come $S = \sqrt{S^2}$.</p><a href=#teorema-524><h3 id=teorema-524><span class=hanchor arialabel=Anchor># </span>Teorema 5.2.4</h3></a><p>Sia i valori $x_1, &mldr;, x_n$ e i valori $\overline{x} = (x_1 + &mldr; + x_n)/n$ e $s^2 = (\sum_i (x_1 - \overline{x})^2)/(n-1)$
Allora valgono le seguenti prorpiet√†:</p><ol><li>$$\overline{x} = \arg \min_{a \in \mathbb{R}}\sum_{i=1}^{n}(x_i - a)^2$$</li><li>$$(n-1)s^2 = \sum_{i=1}^{n}x_i^2 - n\overline{x}$$</li></ol><a href=#proof><h4 id=proof><span class=hanchor arialabel=Anchor># </span>Proof</h4></a><p>$$\begin{align*}
\sum_{i=1}^{n}(x_i - a)^2
&= \sum_{i=1}^{n}(x_i - \overline{x} + \overline{x} - a)^2\\&= \sum_{i=1}^{n}((x_i - \overline{x}) + (\overline{x} - a))^2\\&= \sum_{i=1}^{n}(x-\overline{x})^2 + 2\sum_{i=1}^{n}(x_i - \overline{x})(\overline{x} - a) + \sum_{i=1}^{n}(\overline{x} - a)^2
\end{align*}$$
e tale quantit√† √® minimizzata per $a = \overline{x}$.</p><p>$$\begin{align*}
(n-1)s^2
&= \sum_{i=1}^{n}(x_i - \overline{x})^2\\&= \sum_{i=1}^{n}x_i^2 - 2\sum_{i=1}^{n}x_i\overline{x} + \sum_{i=1}^{n}\overline{x}^2\\&= \sum_{i=1}^{n}x_i^2 - \frac{2}{n}\sum_{i=1}^{n}x_i(x_1 + &mldr; + x_i + &mldr; + x_n) + n\overline{x}^2\\&= \sum_{i=1}^{n}x_i^2 - \frac{2}{n}\sum_{i=1}^{n}(x_ix_1 + &mldr; + x_i^2 + &mldr; + x_ix_n) + n\overline{x}^2\\&= \sum_{i=1}^{n}x_i^2 - \frac{2}{n}\sum_{i=1}^{n}\left[ x_i^2 + \sum_{j \neq i} x_ix_j \right] + n\overline{x}^2\\&= \sum_{i=1}^{n}x_i^2 - \frac{2}{n}\underbrace{\left[\sum_{i=1}^{n} x_i^2 + 2\sum_{j \neq i} x_ix_j \right]}<em>{(x_1 + &mldr; +x_n)^2 = \overline{x}^2n^2} + n\overline{x}^2\\&= \sum</em>{i=1}^{n}x_i^2 - 2n\overline{x}^2 + n\overline{x}^2\\&= \sum_{i=1}^{n}x_i^2 - n\overline{x}^2 ;; \square
\end{align*}$$</p><a href=#lemma-525---media-e-lemma><h3 id=lemma-525---media-e-lemma><span class=hanchor arialabel=Anchor># </span>Lemma 5.2.5 - media e lemma</h3></a><p>Sia $X_1, &mldr;, X_n$ un random sampling, e sia una funzione $g(x)$ tale che $\mathbb{E}\left[ g(X_i)\right]$ e $\text{Var}(g(X_i))$ esistono.
Allora avremo che</p><ol><li>$$\mathbb{E}\left[ \sum_{i=1}^{n}g(X_i) \right] = n\mathbb{E}\left[ g(X_1) \right]$$</li><li>$$\text{Var}\left( \sum_{i=1}^{n}g(X_i) \right) = n\text{Var}\left( g(X_1) \right)$$</li></ol><a href=#proof-1><h4 id=proof-1><span class=hanchor arialabel=Anchor># </span>Proof</h4></a><p>Per il punto 1 basta sfruttare la <strong>linearit√†</strong>.
Per il punto 2 basta osservare che nel caso di <strong>indipendenza</strong> allora anche la varianza √® lineare $\square$.</p><a href=#teorema-526---propriet√†-importanti><h3 id=teorema-526---propriet√†-importanti><span class=hanchor arialabel=Anchor># </span>Teorema 5.2.6 - Propriet√† importanti</h3></a><p>Sia il random sample $X_1, &mldr;, X_n$ con media $\mu$ e vairanza finita $\sigma^2 &lt; \infty$.
Allora valgono le seguenti propriet√†:</p><ol><li>$$\mathbb{E}\left[ \overline{X} \right] = \mu$$ ^69143c</li><li>$$\text{Var}\left(\overline{X}\right) = \frac{\sigma^2}{n}$$ ^2864bf</li><li>$$\mathbb{E}\left[ S^2 \right] = \sigma^2$$ ^89dd43</li></ol><a href=#proof-2><h4 id=proof-2><span class=hanchor arialabel=Anchor># </span>Proof</h4></a><p>Per il
<a href=/ISTI/Random-Sample/ rel=noopener class=internal-link data-src=/ISTI/Random-Sample/>punto 1</a> sfruttiamo la linearit√† del valore atteso
$$\mathbb{E}\left[ \overline{X} \right] = \mathbb{E}\left[ \frac{1}{n}\sum_{i=1}^{n}X_i \right] = \frac{1}{n}n\mathbb{E}\left[ X_1 \right] = \mu $$</p><p>Analogamente per il
<a href=/ISTI/Random-Sample/ rel=noopener class=internal-link data-src=/ISTI/Random-Sample/>punto 2</a>
$$\text{Var}\left( \overline{X} \right) = \text{Var}\left( \frac{1}{n} \sum_{i=1}^{n} X_i \right) = \frac{1}{n^2} \text{Var}\left( \sum_{i=1}^{n} X_i \right) = \frac{1}{n^2}n \text{Var}\left( X_1 \right) = \frac{\sigma^2}{n}$$</p><p>In fine per il
<a href=/ISTI/Random-Sample/ rel=noopener class=internal-link data-src=/ISTI/Random-Sample/>punto 3</a> sfruttiamo il
<a href=/ISTI/Random-Sample/ rel=noopener class=internal-link data-src=/ISTI/Random-Sample/>Teorema 5 2 4</a>
$$\begin{align*}
\mathbb{E}\left[ S^2 \right]
&= \mathbb{E}\left[ \frac{1}{n-1} \sum_{i=1}^{n}(X_i - \overline{X})^2 \right]\\&= \frac{1}{n-1} \mathbb{E}\left[ \sum_{i=1}^{n}X_i^2 - n\overline{X}^2 \right]\\&= \frac{1}{n-1} \left( n\mathbb{E}\left[X_1^2\right] - n\mathbb{E}\left[\overline{X}^2 \right] \right)
\end{align*}$$
Osserviamo che
$$\text{Var}(X) = \mathbb{E}\left[X^2\right] - \mathbb{E}^2\left[X\right] \implies \sigma^2 = \mathbb{E}\left[X^2\right] - \mu^2 \implies \mathbb{E}\left[X^2\right] = \sigma^2 + \mu^2$$
Perci√≤ sfruttando i punti
<a href=/ISTI/Random-Sample/ rel=noopener class=internal-link data-src=/ISTI/Random-Sample/>1</a> e
<a href=/ISTI/Random-Sample/ rel=noopener class=internal-link data-src=/ISTI/Random-Sample/>2</a> avremo che
$$\begin{align*}
\mathbb{E}\left[ S^2 \right]
&= \frac{1}{n-1} \left( n(\sigma^2 - \mu^2) - n\left(\frac{\sigma^2}{n} - \mu^2 \right) \right)\\&=\frac{n}{n-1} \left( \sigma^2 - \cancel\mu^2 - \frac{\sigma^2}{n} + \cancel\mu^2 \right)\\&=\frac{n}{n-1} \left( \sigma^2 - \frac{\sigma^2}{n} \right)\\&= \sigma^2 ;; \square
\end{align*}$$</p><a href=#oservazione><h4 id=oservazione><span class=hanchor arialabel=Anchor># </span>Oservazione</h4></a><p>Il punto
<a href=/ISTI/Random-Sample/ rel=noopener class=internal-link data-src=/ISTI/Random-Sample/>punto 3</a> spiegato il perch√© nella definizione di $S^2$ si divide per $n-1$ e non per $n$.</p><hr><a href=#teorema-527---fgm-della-media-campionaria><h2 id=teorema-527---fgm-della-media-campionaria><span class=hanchor arialabel=Anchor># </span>Teorema 5.2.7 - FGM della media campionaria</h2></a><p>Sia $X_1,&mldr;,X_n$ un <em>random sample</em>, dove ogni v.a. ha
<a href=/ISTI/Distribuzioni-Multivariate/ rel=noopener class=internal-link data-src=/ISTI/Distribuzioni-Multivariate/>fgm</a> $M_X(t)$.
Allora la <em>fgm</em> della media campionaria $\overline{X}$ risulter√† essere $$M_{\overline{X}}(t) = \left[ M_X(t/n) \right]^n$$ dove $X$ √® un qualsiasi $X_i$ (tanto sono i.i.d.).</p><a href=#proof-3><h3 id=proof-3><span class=hanchor arialabel=Anchor># </span>Proof</h3></a><p>Sfruttando il fatto che $X_1,&mldr;,X_n$ sono i.i.d. otteremo $$M_{\overline{X}}(t) = \mathbb{E}\left[ e^{t \overline{X}} \right] = \mathbb{E}\left[ e^{t (X_1 + &mldr; + X_n)/n} \right] = \mathbb{E}\left[ \prod_{i=1}^{n} e^{(t/n)X_i} \right] = \prod_{i=1}^{n} M_{X_i}(t/n) = \left[ M_X(t/n) \right] ; ; \square$$</p><a href=#esempio---normale><h3 id=esempio---normale><span class=hanchor arialabel=Anchor># </span>Esempio - Normale</h3></a><p>Siano $X_1, &mldr;, X_n$ un campio di normali $N(\mu, \sigma^2)$.
La funzione generatrice dei momenti della media campionaria sar√† quindi
$$\begin{align*}
M_{\overline{X}}(t)
&= \left[ M_X(t/n) \right]^n\\&= \left[ \mathbb{E}\left[ e^{(t/n) \cdot N(\mu, \sigma^2)} \right] \right]^n\\&= \left[ \exp{\left( \mu (t/n) + \sigma^2 \frac{(t/n)^2}{2}\right)} \right]^n\\&= \exp{\left( n \left( \mu (t/n) + \sigma^2 \frac{(t/n)^2}{2}\right) \right)}\\&= \exp{\left( \mu t + (\sigma^2/n) \frac{t^2}{2} \right)}
\end{align*}$$</p><p>Perci√≤ $\overline{X}$ ha una distribuzione $N(\mu, \sigma^2/n)$.</p><p>Osserviamo dal
<a href=/ISTI/Random-Sample/ rel=noopener class=internal-link data-src=/ISTI/Random-Sample/>teorema 5.6.2</a> che la distribuzione normale che descrive $\overline{X}$ ha appunto parametri $\mathbb{E}\left[\overline{X}\right]$ e $\text{Var}(\overline{X})$.</p><hr><a href=#teorema-529---convolution-formula><h2 id=teorema-529---convolution-formula><span class=hanchor arialabel=Anchor># </span>Teorema 5.2.9. - Convolution formula</h2></a><p>Siano $X, Y$ due v.a. <strong>cpntinue</strong>, <strong>indipendenti</strong> e con <em>pdf</em> relativamente $f_X(x)$ e $f_Y(y)$.
Sia $Z = X + Y$, allora la sua pdf sar√†
$$f_Z(z) = \int_{-\infty}^{\infty} f_X(w) f_Y(z-w) , dw$$</p><a href=#proof-4><h4 id=proof-4><span class=hanchor arialabel=Anchor># </span>Proof</h4></a><p>Sia $W = X$.
Il <em>Jacobiano</em> della trasformazione da $(X, Y)$ a $(Z, W)$ sar√† 1.
Infatti avremo che $Z = g_1(X,Y) = X+Y$ e $W = g_2(X,Y) = X$ con <strong>inverse</strong>
$$\begin{align*}
X = g^{-1}_1(Z, W) &= W\\Y = g^{-1}_2(Z, W) &= Z-W
\end{align*}$$</p><p>Infatti
$$J = \left\vert
\begin{array}{c}
\dfrac{\partial}{\partial Z} W & \dfrac{\partial}{\partial W} W\\\dfrac{\partial}{\partial Z} Z-W & \dfrac{\partial}{\partial W} Z-W
\end{array}
\right\vert
= -1$$</p><p>Perci√≤ la distribuzione congiunta di $Z,W$ risulter√† essere $$f_{Z,W}(z, w) = f_{X,Y}(w, z-w) \vert J \vert = f_{X}(w)f_Y(z-w)$$
Per ottenere la distribuzione marginale di $Z$ basta fissare un valore di $z$ e integrare su $w$
$$f_Z(z) = \int_{-\infty}^{\infty} f_{Z, W}(z, w) , dw = \int_{-\infty}^{\infty} f_X(w)f_Y(z-w) , dw ;; \square$$</p><a href=#esempio---somma-di-va-di-cauchy><h3 id=esempio---somma-di-va-di-cauchy><span class=hanchor arialabel=Anchor># </span>Esempio - somma di v.a. di Cauchy</h3></a><p>Sia il campionamento $Z_1, &mldr;, Z_n$ campionati da una $\text{Cauchy}(0,1)$.
Ricordiamo che $$\text{Cauchy}(x_0, \gamma) = \frac{1}{\pi} \frac{\gamma}{(x- x_0)^2 + \gamma^2}$$ perci√≤ $$Z_i \sim \text{Cauchy}(0,1) = \frac{1}{\pi} \frac{1}{x^2 + 1}$$
<a href=/ISTI/Distribuzioni/ rel=noopener class=internal-link data-src=/ISTI/Distribuzioni/>Sappiamo</a> che la somma di $n$ $\text{Cauchy}(0,1)$ √® ancora una $\text{Cauchy}(0, n)$.
$$Z = \sum_{i=1}^{n}Z_i \sim \text{Cauchy}(0,n) \implies f_Z(z) = \frac{1}{n\pi}\frac{1}{(z/n)^2 + 1}$$
Dato che $\overline{Z} = g(Z) = Z/n$, e dato $Z = g^{-1}(\overline{Z}) = n\overline{Z}$, avremo che la distribuzione della media campionaria $\overline{Z}$ √® <u>ancora</u> una $\text{Cauchy}(0,1)$!
Infatti $$f_{\overline{Z}}(z) = f_Z(g^{-1}(z)) \cdot \left\vert \frac{d}{dz} g^{-1}(z) \right\vert = f_Z(nz) \cdot n = \cancel{n}\frac{1}{\cancel{n}\pi}\frac{1}{\left( \cfrac{\cancel{n}z}{\cancel{n}}\right)^2 + 1} = \frac{1}{\pi}\frac{1}{z^2 + 1} = \text{Cauchy}(0,1)$$</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://Alessandrostr95.github.io/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Alessandro Straziota using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, ¬© 2022</p><ul><li><a href=https://Alessandrostr95.github.io/>Home</a></li><li><a href=https://twitter.com/Alessandro_357>Twitter</a></li><li><a href=https://github.com/Alessandrostr95>Github</a></li></ul></footer></div></div></body></html>